{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c146cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f4b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnafm(device=None):\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\User\\RNA-FM-main\")  # –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ, –≥–¥–µ –ª–µ–∂–∏—Ç fm\n",
    "\n",
    "    from fm import pretrained  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    return model, batch_converter, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017cf8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–∞–∫–µ—Ç 'fm' —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω!\n",
      "‚úÖ –ú–æ–¥–µ–ª—å RNA-FM –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞! –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import ptflops\n",
    "# –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è RNA-FM (–≥–¥–µ –ª–µ–∂–∏—Ç –ø–∞–ø–∫–∞ fm)\n",
    "RNAFM_PATH = Path(r\"C:\\Users\\User\\RNA-FM-main\")\n",
    "sys.path.insert(0, str(RNAFM_PATH))\n",
    "\n",
    "try:\n",
    "    from fm import pretrained\n",
    "    print(\"‚úÖ –ü–∞–∫–µ—Ç 'fm' —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω!\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ 'fm':\", e)\n",
    "    raise\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "try:\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    print(f\"‚úÖ –ú–æ–¥–µ–ª—å RNA-FM –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞! –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa0d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnafm(device=None):\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\User\\RNA-FM-main\")  # –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ, –≥–¥–µ –ª–µ–∂–∏—Ç fm\n",
    "\n",
    "    from fm import pretrained  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    return model, batch_converter, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f070a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==== —É–¥–æ–±—Å—Ç–≤–∞ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å ====\n",
    "import math, random, os\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94c7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\–ò–¢–ú–û\\–ü—Ä–æ–µ–∫—Ç\\—Ä–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π DT_Curated\\Data_ML.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e25eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- –∫–æ–Ω—Ñ–∏–≥ (–æ—Å—Ç–∞–≤—å —Å–≤–æ–∏ –ø—É—Ç–∏/–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –µ—Å—Ç—å) ----\n",
    "DATA_DIR   = Path(r\"C:\\Users\\User\\Desktop\\–ò–¢–ú–û\\–ü—Ä–æ–µ–∫—Ç\\—Ä–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π DT_Curated\")\n",
    "CSV_NAME   = \"Data_ML.csv\"\n",
    "EMB_NPY    = \"embeddings.npy\"\n",
    "MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP      = torch.cuda.is_available()\n",
    "EPOCHS   = 1\n",
    "BATCH_SIZE = 8         # –º–æ–∂–Ω–æ 8 ‚Äî –±—ã—Å—Ç—Ä–µ–µ —á–µ–º 2, –Ω–æ –≤—Å—ë –µ—â—ë –ª—ë–≥–∫–∏–π\n",
    "MAX_LEN  = 64\n",
    "FREEZE_N = 10\n",
    "NUM_HEADS = 8\n",
    "LR       = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_FR = 0.06\n",
    "SEED     = 42\n",
    "\n",
    "# ---- —Ñ–∏–∫—Å–∞—Ü–∏—è —Å–∏–¥–æ–≤ ----\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9a2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- –¥–∞—Ç–∞—Å–µ—Ç (SMILES + RNA —ç–º–±–µ–¥–¥–∏–Ω–≥–∏) ----\n",
    "class SmilesRnaDataset(Dataset):\n",
    "    def __init__(self, smiles: List[str], rna_embs: np.ndarray, tokenizer, max_len: int):\n",
    "        assert len(smiles) == len(rna_embs), f\"–†–∞–∑–Ω—ã–µ –¥–ª–∏–Ω—ã: {len(smiles)} vs {len(rna_embs)}\"\n",
    "        self.smiles = smiles\n",
    "        self.rna = torch.tensor(rna_embs, dtype=torch.float32)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.smiles[idx]\n",
    "        enc = self.tok(\n",
    "            s,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)         # (L,)\n",
    "        attention = enc[\"attention_mask\"].squeeze(0)    # (L,)\n",
    "        rna = self.rna[idx]                              # (d_rna,)\n",
    "\n",
    "        # labels = input_ids, –Ω–æ –ø–∞–¥–¥–∏–Ω–≥–∏ -> -100\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention,\n",
    "            \"rna\": rna,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6588e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- –º–æ–¥–µ–ª—å: ChemBERTa + cross-attn (—Ç–≤–æ—è –≤–µ—Ä—Å–∏—è; forward –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç logits) ----\n",
    "class ChemBERTaCrossAttentionLM(nn.Module):\n",
    "    def __init__(self, model_name: str, rna_dim: int, freeze_n_layers: int = 10, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.chem = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.chem.config.hidden_size\n",
    "\n",
    "        # –∑–∞–º–æ—Ä–æ–∑–∫–∞ –ø–µ—Ä–≤—ã—Ö N —Å–ª–æ—ë–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å encoder.layer)\n",
    "        if hasattr(self.chem, \"encoder\") and hasattr(self.chem.encoder, \"layer\"):\n",
    "            n_layers = len(self.chem.encoder.layer)\n",
    "            freeze_n = min(freeze_n_layers, n_layers)\n",
    "            for layer in self.chem.encoder.layer[:freeze_n]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        self.rna_proj = nn.Linear(rna_dim, hidden)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden * 4, hidden)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(hidden, self.chem.config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, rna):\n",
    "        enc = self.chem(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        h = enc.last_hidden_state                       # (B, L, H)\n",
    "\n",
    "        # RNA -> (B, 1, H)\n",
    "        rna_tok = self.rna_proj(rna).unsqueeze(1)\n",
    "\n",
    "        # cross-attn: Q=h(L —Ç–æ–∫–µ–Ω–æ–≤), K/V=–æ–¥–∏–Ω RNA —Ç–æ–∫–µ–Ω\n",
    "        attn_out, _ = self.cross_attn(query=h, key=rna_tok, value=rna_tok)  # (B, L, H)\n",
    "\n",
    "        # residual + FFN\n",
    "        h = self.norm(h + attn_out)\n",
    "        h = h + self.ffn(h)\n",
    "\n",
    "        logits = self.lm_head(h)                        # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 tokenizer,\n",
    "                 rna: torch.Tensor,\n",
    "                 max_new_tokens: int = 64,\n",
    "                 prefix: Optional[str] = None,\n",
    "                 temperature: float = 1.0,\n",
    "                 top_k: Optional[int] = None,\n",
    "                 top_p: Optional[float] = 0.9):\n",
    "        \"\"\"\n",
    "        –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (encoder-conditioned).\n",
    "        - –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ attention_mask;\n",
    "        - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ max_new_tokens;\n",
    "        - temperature, top_k, top_p (nucleus).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            # Roberta-–ø–æ–¥–æ–±–Ω—ã–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç eos –∫–∞–∫ pad\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        if prefix is None:\n",
    "            if tokenizer.bos_token_id is not None:\n",
    "                ids = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "            else:\n",
    "                # –µ—Å–ª–∏ BOS –Ω–µ—Ç ‚Äî –Ω–∞—á–Ω—ë–º —Å –ø—É—Å—Ç–æ–≥–æ (–¥–æ–±–∞–≤–∏–º –ø–∞–¥–¥–∏–Ω–≥-–º–∞—Å–∫—É)\n",
    "                ids = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
    "        else:\n",
    "            ids = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).input_ids.to(device)\n",
    "\n",
    "        rna = rna.to(device)  # (B, d_rna) –∏–ª–∏ (1, d_rna)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–∞–¥–¥–∏–Ω–≥=0)\n",
    "            attn = (ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            logits = self.forward(ids, attn, rna.expand(ids.size(0), -1))\n",
    "            next_logits = logits[:, -1, :]  # (B, V)\n",
    "\n",
    "            # temperature\n",
    "            if temperature and temperature > 0:\n",
    "                next_logits = next_logits / temperature\n",
    "\n",
    "            # top-k\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, next_logits.size(-1))\n",
    "                v, _ = torch.topk(next_logits, k)\n",
    "                thr = v[:, -1].unsqueeze(-1)\n",
    "                next_logits = torch.where(next_logits < thr, torch.full_like(next_logits, -1e10), next_logits)\n",
    "\n",
    "            # top-p (nucleus)\n",
    "            if top_p is not None and 0 < top_p < 1.0:\n",
    "                sorted_logits, sorted_idx = torch.sort(next_logits, descending=True, dim=-1)\n",
    "                probs = torch.softmax(sorted_logits, dim=-1)\n",
    "                cumprobs = torch.cumsum(probs, dim=-1)\n",
    "                # –º–∞—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã—Ö–æ–¥—è—â–∏—Ö –∑–∞ –ø—Ä–µ–¥–µ–ª –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–∞—Å—Å—ã top_p\n",
    "                cutoff = (cumprobs > top_p)\n",
    "                # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω –æ—Å—Ç–∞—ë—Ç—Å—è\n",
    "                cutoff[..., 0] = False\n",
    "                sorted_logits[cutoff] = -1e10\n",
    "                # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏\n",
    "                unsorted = torch.full_like(next_logits, -1e10)\n",
    "                unsorted.scatter_(1, sorted_idx, sorted_logits)\n",
    "                next_logits = unsorted\n",
    "\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            ids = torch.cat([ids, next_id], dim=1)\n",
    "\n",
    "            # —Å—Ç–æ–ø –ø–æ EOS\n",
    "            if tokenizer.eos_token_id is not None and (next_id == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "            # —Å—Ç–æ–ø, –µ—Å–ª–∏ —É–ø—ë—Ä–ª–∏—Å—å –≤ MAX_LEN\n",
    "            if ids.size(1) >= MAX_LEN:\n",
    "                break\n",
    "\n",
    "        return tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc218ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ====\n",
    "csv_path = DATA_DIR / CSV_NAME\n",
    "emb_path = DATA_DIR / EMB_NPY\n",
    "\n",
    "df_full = pd.read_csv(csv_path)\n",
    "assert \"SMILES\" in df_full.columns, \"–í CSV –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∞ 'SMILES'.\"\n",
    "rna_embs_full = np.load(emb_path)\n",
    "\n",
    "if rna_embs_full.shape[0] != len(df_full):\n",
    "    raise ValueError(f\"RNA-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {rna_embs_full.shape[0]}, —Å—Ç—Ä–æ–∫ –≤ CSV: {len(df_full)} ‚Äî –ø—Ä–æ–≤–µ—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d25fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 150 | Val: 6089\n"
     ]
    }
   ],
   "source": [
    "# ---- –¥–µ–ª–∏–º 80/20, –∞ train –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—Ä–µ–∑–∞–µ–º –¥–æ 150 —Å—Ç—Ä–æ–∫ ----\n",
    "df_train, df_val, embs_train, embs_val = train_test_split(\n",
    "    df_full, rna_embs_full, test_size=0.2, random_state=SEED, shuffle=True, stratify=None\n",
    ")\n",
    "if len(df_train) > 150:\n",
    "    df_train = df_train.iloc[:150].reset_index(drop=True)\n",
    "    embs_train = embs_train[:150]\n",
    "\n",
    "print(f\"Train: {len(df_train)} | Val: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607f1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81caaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –†–∞–∑–±–∏–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ: train=120 | val=30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "rna_embs= np.load(r\"C:\\Users\\User\\Desktop\\–ò–¢–ú–û\\–ü—Ä–æ–µ–∫—Ç\\—Ä–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π DT_Curated\\embeddings.npy\")\n",
    "# --- –ë–µ—Ä—ë–º —Ä–æ–≤–Ω–æ 150 –ø–µ—Ä–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–µ–∂–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏) ---\n",
    "N = min(150, len(df), rna_embs.shape[0])\n",
    "df_small   = df.iloc[:N].reset_index(drop=True)\n",
    "rna_small  = rna_embs[:N]\n",
    "smiles_all = df_small[\"SMILES\"].astype(str)\n",
    "\n",
    "# --- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è 80/20 ---\n",
    "idx = np.arange(N)\n",
    "train_idx, val_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# --- –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏/–º–∞—Å—Å–∏–≤—ã –ø–æ–¥ —Ç–≤–æ–π SmilesRnaDataset ---\n",
    "smiles_train = smiles_all.iloc[train_idx].tolist()\n",
    "smiles_val   = smiles_all.iloc[val_idx].tolist()\n",
    "rna_train    = rna_small[train_idx]\n",
    "rna_val      = rna_small[val_idx]\n",
    "\n",
    "# --- –î–∞—Ç–∞—Å–µ—Ç—ã ---\n",
    "train_ds = SmilesRnaDataset(smiles_train, rna_train, tokenizer, MAX_LEN)\n",
    "val_ds   = SmilesRnaDataset(smiles_val,   rna_val,   tokenizer, MAX_LEN)\n",
    "\n",
    "# --- DataLoader-—ã (Windows-safe: num_workers=0) ---\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ –†–∞–∑–±–∏–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ: train={len(train_ds)} | val={len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72f65be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- –º–æ–¥–µ–ª—å ----\n",
    "d_rna = embs_train.shape[1]\n",
    "model = ChemBERTaCrossAttentionLM(MODEL_NAME, rna_dim=d_rna, freeze_n_layers=FREEZE_N, num_heads=NUM_HEADS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7823c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11980\\3237296865.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n"
     ]
    }
   ],
   "source": [
    "# ---- –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä/—à–µ–¥—É–ª–µ—Ä/–ª–æ—Å—Å—ã ----\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = max(1, EPOCHS * len(train_dl))\n",
    "warmup_steps = int(WARMUP_FR * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "loss_f = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e3cbaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –≠–ø–æ—Ö–∞ 1/1 (train 120 –ø—Ä–∏–º–µ—Ä–æ–≤) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11980\\3554425788.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/15] loss: 6.8863\n",
      "üìä –°—Ä–µ–¥–Ω–∏–π train loss: 6.8833\n",
      "‚úÖ –í–∞–ª–∏–¥. –º–µ—Ç—Ä–∏–∫–∏: Loss=6.9463 | PPL=1039.25 | Acc=0.00%\n"
     ]
    }
   ],
   "source": [
    "# === –û–±—É—á–µ–Ω–∏–µ (1 —ç–ø–æ—Ö–∞ –Ω–∞ train_dl) ===\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "print(f\"\\n=== –≠–ø–æ—Ö–∞ 1/1 (train {len(train_ds)} –ø—Ä–∏–º–µ—Ä–æ–≤) ===\")\n",
    "for step, batch in enumerate(train_dl, 1):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "    rna       = batch[\"rna\"].to(DEVICE)\n",
    "    labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast(enabled=AMP):\n",
    "        logits = model(input_ids, attention, rna)\n",
    "        logits_shift = logits[:, :-1, :].contiguous()\n",
    "        labels_shift = labels[:, 1:].contiguous()\n",
    "        loss = loss_f(\n",
    "            logits_shift.view(-1, logits_shift.size(-1)),\n",
    "            labels_shift.view(-1)\n",
    "        )\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, GRAD_CLIP)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[{step}/{len(train_dl)}] loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"üìä –°—Ä–µ–¥–Ω–∏–π train loss: {running_loss / max(1, len(train_dl)):.4f}\")\n",
    "\n",
    "# === –í–∞–ª–∏–¥–∞—Ü–∏—è (–Ω–∞ val_dl) ===\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_tok  = 0\n",
    "correct    = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "        rna       = batch[\"rna\"].to(DEVICE)\n",
    "        labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention, rna)\n",
    "        logits_shift = logits[:, :-1, :].contiguous()\n",
    "        labels_shift = labels[:, 1:].contiguous()\n",
    "\n",
    "        mask = labels_shift.ne(-100)\n",
    "        loss = loss_f(\n",
    "            logits_shift.view(-1, logits_shift.size(-1)),\n",
    "            labels_shift.view(-1)\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        total_tok  += mask.sum().item()\n",
    "        preds = logits_shift.argmax(dim=-1)\n",
    "        correct += ((preds == labels_shift) & mask).sum().item()\n",
    "\n",
    "val_loss = total_loss / max(1, total_tok)\n",
    "val_ppl  = float(np.exp(val_loss)) if total_tok > 0 else float(\"inf\")\n",
    "val_acc  = correct / max(1, total_tok)\n",
    "\n",
    "print(f\"‚úÖ –í–∞–ª–∏–¥. –º–µ—Ç—Ä–∏–∫–∏: Loss={val_loss:.4f} | PPL={val_ppl:.2f} | Acc={val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f45b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: C:\\Users\\User\\Desktop\\–ò–¢–ú–û\\–ü—Ä–æ–µ–∫—Ç\\—Ä–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π DT_Curated\\chemberta_crossattn_rna\n"
     ]
    }
   ],
   "source": [
    "# ==== —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ====\n",
    "out_dir = DATA_DIR / \"chemberta_crossattn_rna\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "# —Å–æ—Ö—Ä–∞–Ω–∏–º –≤–µ—Å–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ HF\n",
    "torch.save(model.state_dict(), out_dir / \"pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12c5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self,\n",
    "             tokenizer,\n",
    "             rna: torch.Tensor,\n",
    "             max_new_tokens: int = 64,\n",
    "             prefix: str = None,\n",
    "             temperature: float = 1.0,\n",
    "             top_k: int = None,\n",
    "             top_p: float = None):\n",
    "    self.eval()\n",
    "    device = next(self.parameters()).device\n",
    "\n",
    "    # BOS/–ø—Ä–µ—Ñ–∏–∫—Å\n",
    "    if prefix is None:\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            # –¥–ª—è Roberta-–ø–æ–¥–æ–±–Ω—ã—Ö —á–∞—Å—Ç–æ <s> –∫–∞–∫ bos\n",
    "            bos_id = tokenizer.convert_tokens_to_ids(\"<s>\") if \"<s>\" in tokenizer.get_vocab() else tokenizer.cls_token_id\n",
    "        ids = torch.tensor([[bos_id]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        enc = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN, add_special_tokens=True)\n",
    "        ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    rna = rna.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            attn_mask = (ids != (tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0)).long()\n",
    "            logits = self.forward(ids, attn_mask, rna.expand(ids.size(0), -1))\n",
    "            next_logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "\n",
    "            # top-k\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, next_logits.size(-1))\n",
    "                v, _ = torch.topk(next_logits, k)\n",
    "                cut = v[:, [-1]]\n",
    "                next_logits[next_logits < cut] = -float(\"inf\")\n",
    "\n",
    "            # top-p (nucleus)\n",
    "            if top_p is not None and 0 < top_p < 1:\n",
    "                sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)\n",
    "                probs = torch.softmax(sorted_logits, dim=-1)\n",
    "                cumprobs = torch.cumsum(probs, dim=-1)\n",
    "                # –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å—ë –ø–æ—Å–ª–µ –ø–æ—Ä–æ–≥–∞ p\n",
    "                sorted_logits[cumprobs > top_p] = -float(\"inf\")\n",
    "                # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫\n",
    "                next_logits = torch.zeros_like(next_logits).scatter(1, sorted_idx, sorted_logits)\n",
    "\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "\n",
    "            ids = torch.cat([ids, next_id], dim=1)\n",
    "\n",
    "            # –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ EOS\n",
    "            if tokenizer.eos_token_id is not None and (next_id == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "    return tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b00ff999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Validation Loss: 6.9463\n",
      "üìâ Perplexity: 1039.2538\n",
      "üéØ Token Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# ==== –≤–∞–ª–∏–¥–∞—Ü–∏—è (–º–µ—Ç—Ä–∏–∫–∏) ====\n",
    "def evaluate(model, dataloader, loss_f):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "            rna       = batch[\"rna\"].to(DEVICE)\n",
    "            labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention, rna)\n",
    "            logits_shift = logits[:, :-1, :].contiguous()\n",
    "            labels_shift = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss = loss_f(logits_shift.view(-1, logits_shift.size(-1)),\n",
    "                          labels_shift.view(-1))\n",
    "\n",
    "            mask = labels_shift != -100\n",
    "            n_tok = mask.sum().item()\n",
    "            total_loss += loss.item() * n_tok\n",
    "            total_tokens += n_tok\n",
    "\n",
    "            preds = logits_shift.argmax(dim=-1)\n",
    "            correct += ((preds == labels_shift) & mask).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 50 else float(\"inf\")\n",
    "    acc = correct / max(1, total_tokens)\n",
    "    return avg_loss, ppl, acc\n",
    "\n",
    "val_loss, val_ppl, val_acc = evaluate(model, val_dl, loss_f)\n",
    "print(f\"\\nüìà Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"üìâ Perplexity: {val_ppl:.4f}\")\n",
    "print(f\"üéØ Token Accuracy: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adf0c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ –ü—Ä–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:\n",
      "CCCCCC~ÔøΩccncFCCCCNCcncscSHCCCCOcnncCCOCCOCÔøΩÔøΩsnSHSHÔøΩ3CCCOCCCCOCCNC)/ncscCCOCCOC\u0007tÔøΩRCCONCÔøΩCCOcjCOPÔøΩOCCCnCSCCCNCOCCnCSCCOOCCCnTCSCCOCÔøΩCCOCCNCCCOcSHccccnRccocÔøΩTNCCCCCCÔøΩÔøΩOCCCNCccccnÔøΩÔøΩ43ClCCBrCCNCCCCCCCSCCOCÔøΩccoc\n"
     ]
    }
   ],
   "source": [
    "# ==== –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ ====\n",
    "model.eval()\n",
    "rna_t = torch.tensor(embs_val[0], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "sample = model.generate(\n",
    "    tokenizer,\n",
    "    rna=rna_t,\n",
    "    max_new_tokens=64,\n",
    "    prefix=None,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(\"\\nüß™ –ü—Ä–æ–±–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è:\")\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
