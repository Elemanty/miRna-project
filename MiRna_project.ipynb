{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c146cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f4b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnafm(device=None):\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\User\\RNA-FM-main\")  # путь к папке, где лежит fm\n",
    "\n",
    "    from fm import pretrained  # локальный импорт\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    return model, batch_converter, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017cf8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Пакет 'fm' успешно импортирован!\n",
      "✅ Модель RNA-FM загружена и готова! Устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import ptflops\n",
    "# Путь к корню репозитория RNA-FM (где лежит папка fm)\n",
    "RNAFM_PATH = Path(r\"C:\\Users\\User\\RNA-FM-main\")\n",
    "sys.path.insert(0, str(RNAFM_PATH))\n",
    "\n",
    "try:\n",
    "    from fm import pretrained\n",
    "    print(\"✅ Пакет 'fm' успешно импортирован!\")\n",
    "except ImportError as e:\n",
    "    print(\"❌ Ошибка импорта 'fm':\", e)\n",
    "    raise\n",
    "\n",
    "# Определяем устройство\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "try:\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    print(f\"✅ Модель RNA-FM загружена и готова! Устройство: {device}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Ошибка при загрузке модели:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa0d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnafm(device=None):\n",
    "    import sys\n",
    "    sys.path.append(r\"C:\\Users\\User\\RNA-FM-main\")  # путь к папке, где лежит fm\n",
    "\n",
    "    from fm import pretrained  # локальный импорт\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, alphabet = pretrained.rna_fm_t12()\n",
    "    model = model.to(device).eval()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    return model, batch_converter, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f070a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==== удобства и стабильность ====\n",
    "import math, random, os\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94c7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\ИТМО\\Проект\\работа с базой DT_Curated\\Data_ML.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e25eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- конфиг (оставь свои пути/параметры как есть) ----\n",
    "DATA_DIR   = Path(r\"C:\\Users\\User\\Desktop\\ИТМО\\Проект\\работа с базой DT_Curated\")\n",
    "CSV_NAME   = \"Data_ML.csv\"\n",
    "EMB_NPY    = \"embeddings.npy\"\n",
    "MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "AMP      = torch.cuda.is_available()\n",
    "EPOCHS   = 1\n",
    "BATCH_SIZE = 8         # можно 8 — быстрее чем 2, но всё ещё лёгкий\n",
    "MAX_LEN  = 64\n",
    "FREEZE_N = 10\n",
    "NUM_HEADS = 8\n",
    "LR       = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_FR = 0.06\n",
    "SEED     = 42\n",
    "\n",
    "# ---- фиксация сидов ----\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9a2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- датасет (SMILES + RNA эмбеддинги) ----\n",
    "class SmilesRnaDataset(Dataset):\n",
    "    def __init__(self, smiles: List[str], rna_embs: np.ndarray, tokenizer, max_len: int):\n",
    "        assert len(smiles) == len(rna_embs), f\"Разные длины: {len(smiles)} vs {len(rna_embs)}\"\n",
    "        self.smiles = smiles\n",
    "        self.rna = torch.tensor(rna_embs, dtype=torch.float32)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.smiles[idx]\n",
    "        enc = self.tok(\n",
    "            s,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)         # (L,)\n",
    "        attention = enc[\"attention_mask\"].squeeze(0)    # (L,)\n",
    "        rna = self.rna[idx]                              # (d_rna,)\n",
    "\n",
    "        # labels = input_ids, но паддинги -> -100\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention,\n",
    "            \"rna\": rna,\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6588e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- модель: ChemBERTa + cross-attn (твоя версия; forward просто возвращает logits) ----\n",
    "class ChemBERTaCrossAttentionLM(nn.Module):\n",
    "    def __init__(self, model_name: str, rna_dim: int, freeze_n_layers: int = 10, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.chem = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.chem.config.hidden_size\n",
    "\n",
    "        # заморозка первых N слоёв (если есть encoder.layer)\n",
    "        if hasattr(self.chem, \"encoder\") and hasattr(self.chem.encoder, \"layer\"):\n",
    "            n_layers = len(self.chem.encoder.layer)\n",
    "            freeze_n = min(freeze_n_layers, n_layers)\n",
    "            for layer in self.chem.encoder.layer[:freeze_n]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "        self.rna_proj = nn.Linear(rna_dim, hidden)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden * 4, hidden)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(hidden, self.chem.config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, rna):\n",
    "        enc = self.chem(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        h = enc.last_hidden_state                       # (B, L, H)\n",
    "\n",
    "        # RNA -> (B, 1, H)\n",
    "        rna_tok = self.rna_proj(rna).unsqueeze(1)\n",
    "\n",
    "        # cross-attn: Q=h(L токенов), K/V=один RNA токен\n",
    "        attn_out, _ = self.cross_attn(query=h, key=rna_tok, value=rna_tok)  # (B, L, H)\n",
    "\n",
    "        # residual + FFN\n",
    "        h = self.norm(h + attn_out)\n",
    "        h = h + self.ffn(h)\n",
    "\n",
    "        logits = self.lm_head(h)                        # (B, L, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 tokenizer,\n",
    "                 rna: torch.Tensor,\n",
    "                 max_new_tokens: int = 64,\n",
    "                 prefix: Optional[str] = None,\n",
    "                 temperature: float = 1.0,\n",
    "                 top_k: Optional[int] = None,\n",
    "                 top_p: Optional[float] = 0.9):\n",
    "        \"\"\"\n",
    "        Автогенерация токенов (encoder-conditioned).\n",
    "        - корректные attention_mask;\n",
    "        - ограничение по max_new_tokens;\n",
    "        - temperature, top_k, top_p (nucleus).\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        if tokenizer.pad_token is None:\n",
    "            # Roberta-подобные часто используют eos как pad\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        if prefix is None:\n",
    "            if tokenizer.bos_token_id is not None:\n",
    "                ids = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "            else:\n",
    "                # если BOS нет — начнём с пустого (добавим паддинг-маску)\n",
    "                ids = torch.tensor([[tokenizer.pad_token_id]], device=device)\n",
    "        else:\n",
    "            ids = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).input_ids.to(device)\n",
    "\n",
    "        rna = rna.to(device)  # (B, d_rna) или (1, d_rna)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # актуальная маска внимания (паддинг=0)\n",
    "            attn = (ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "            logits = self.forward(ids, attn, rna.expand(ids.size(0), -1))\n",
    "            next_logits = logits[:, -1, :]  # (B, V)\n",
    "\n",
    "            # temperature\n",
    "            if temperature and temperature > 0:\n",
    "                next_logits = next_logits / temperature\n",
    "\n",
    "            # top-k\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, next_logits.size(-1))\n",
    "                v, _ = torch.topk(next_logits, k)\n",
    "                thr = v[:, -1].unsqueeze(-1)\n",
    "                next_logits = torch.where(next_logits < thr, torch.full_like(next_logits, -1e10), next_logits)\n",
    "\n",
    "            # top-p (nucleus)\n",
    "            if top_p is not None and 0 < top_p < 1.0:\n",
    "                sorted_logits, sorted_idx = torch.sort(next_logits, descending=True, dim=-1)\n",
    "                probs = torch.softmax(sorted_logits, dim=-1)\n",
    "                cumprobs = torch.cumsum(probs, dim=-1)\n",
    "                # маска токенов, выходящих за предел вероятностной массы top_p\n",
    "                cutoff = (cumprobs > top_p)\n",
    "                # гарантируем, что хотя бы один токен остаётся\n",
    "                cutoff[..., 0] = False\n",
    "                sorted_logits[cutoff] = -1e10\n",
    "                # возвращаем на исходные позиции\n",
    "                unsorted = torch.full_like(next_logits, -1e10)\n",
    "                unsorted.scatter_(1, sorted_idx, sorted_logits)\n",
    "                next_logits = unsorted\n",
    "\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            ids = torch.cat([ids, next_id], dim=1)\n",
    "\n",
    "            # стоп по EOS\n",
    "            if tokenizer.eos_token_id is not None and (next_id == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "            # стоп, если упёрлись в MAX_LEN\n",
    "            if ids.size(1) >= MAX_LEN:\n",
    "                break\n",
    "\n",
    "        return tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc218ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== загрузка данных ====\n",
    "csv_path = DATA_DIR / CSV_NAME\n",
    "emb_path = DATA_DIR / EMB_NPY\n",
    "\n",
    "df_full = pd.read_csv(csv_path)\n",
    "assert \"SMILES\" in df_full.columns, \"В CSV должна быть колонка 'SMILES'.\"\n",
    "rna_embs_full = np.load(emb_path)\n",
    "\n",
    "if rna_embs_full.shape[0] != len(df_full):\n",
    "    raise ValueError(f\"RNA-эмбеддингов: {rna_embs_full.shape[0]}, строк в CSV: {len(df_full)} — проверь соответствие!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85d25fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 150 | Val: 6089\n"
     ]
    }
   ],
   "source": [
    "# ---- делим 80/20, а train дополнительно урезаем до 150 строк ----\n",
    "df_train, df_val, embs_train, embs_val = train_test_split(\n",
    "    df_full, rna_embs_full, test_size=0.2, random_state=SEED, shuffle=True, stratify=None\n",
    ")\n",
    "if len(df_train) > 150:\n",
    "    df_train = df_train.iloc[:150].reset_index(drop=True)\n",
    "    embs_train = embs_train[:150]\n",
    "\n",
    "print(f\"Train: {len(df_train)} | Val: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607f1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- токенизатор ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81caaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Разбиение готово: train=120 | val=30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "rna_embs= np.load(r\"C:\\Users\\User\\Desktop\\ИТМО\\Проект\\работа с базой DT_Curated\\embeddings.npy\")\n",
    "# --- Берём ровно 150 первых примеров (и синхронно режем эмбеддинги) ---\n",
    "N = min(150, len(df), rna_embs.shape[0])\n",
    "df_small   = df.iloc[:N].reset_index(drop=True)\n",
    "rna_small  = rna_embs[:N]\n",
    "smiles_all = df_small[\"SMILES\"].astype(str)\n",
    "\n",
    "# --- Индексы для разбиения 80/20 ---\n",
    "idx = np.arange(N)\n",
    "train_idx, val_idx = train_test_split(idx, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# --- Формируем списки/массивы под твой SmilesRnaDataset ---\n",
    "smiles_train = smiles_all.iloc[train_idx].tolist()\n",
    "smiles_val   = smiles_all.iloc[val_idx].tolist()\n",
    "rna_train    = rna_small[train_idx]\n",
    "rna_val      = rna_small[val_idx]\n",
    "\n",
    "# --- Датасеты ---\n",
    "train_ds = SmilesRnaDataset(smiles_train, rna_train, tokenizer, MAX_LEN)\n",
    "val_ds   = SmilesRnaDataset(smiles_val,   rna_val,   tokenizer, MAX_LEN)\n",
    "\n",
    "# --- DataLoader-ы (Windows-safe: num_workers=0) ---\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"✅ Разбиение готово: train={len(train_ds)} | val={len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72f65be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- модель ----\n",
    "d_rna = embs_train.shape[1]\n",
    "model = ChemBERTaCrossAttentionLM(MODEL_NAME, rna_dim=d_rna, freeze_n_layers=FREEZE_N, num_heads=NUM_HEADS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7823c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11980\\3237296865.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n"
     ]
    }
   ],
   "source": [
    "# ---- оптимайзер/шедулер/лоссы ----\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = max(1, EPOCHS * len(train_dl))\n",
    "warmup_steps = int(WARMUP_FR * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "loss_f = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e3cbaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Эпоха 1/1 (train 120 примеров) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\my_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11980\\3554425788.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/15] loss: 6.8863\n",
      "📊 Средний train loss: 6.8833\n",
      "✅ Валид. метрики: Loss=6.9463 | PPL=1039.25 | Acc=0.00%\n"
     ]
    }
   ],
   "source": [
    "# === Обучение (1 эпоха на train_dl) ===\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "print(f\"\\n=== Эпоха 1/1 (train {len(train_ds)} примеров) ===\")\n",
    "for step, batch in enumerate(train_dl, 1):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "    rna       = batch[\"rna\"].to(DEVICE)\n",
    "    labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast(enabled=AMP):\n",
    "        logits = model(input_ids, attention, rna)\n",
    "        logits_shift = logits[:, :-1, :].contiguous()\n",
    "        labels_shift = labels[:, 1:].contiguous()\n",
    "        loss = loss_f(\n",
    "            logits_shift.view(-1, logits_shift.size(-1)),\n",
    "            labels_shift.view(-1)\n",
    "        )\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, GRAD_CLIP)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[{step}/{len(train_dl)}] loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"📊 Средний train loss: {running_loss / max(1, len(train_dl)):.4f}\")\n",
    "\n",
    "# === Валидация (на val_dl) ===\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_tok  = 0\n",
    "correct    = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "        rna       = batch[\"rna\"].to(DEVICE)\n",
    "        labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention, rna)\n",
    "        logits_shift = logits[:, :-1, :].contiguous()\n",
    "        labels_shift = labels[:, 1:].contiguous()\n",
    "\n",
    "        mask = labels_shift.ne(-100)\n",
    "        loss = loss_f(\n",
    "            logits_shift.view(-1, logits_shift.size(-1)),\n",
    "            labels_shift.view(-1)\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        total_tok  += mask.sum().item()\n",
    "        preds = logits_shift.argmax(dim=-1)\n",
    "        correct += ((preds == labels_shift) & mask).sum().item()\n",
    "\n",
    "val_loss = total_loss / max(1, total_tok)\n",
    "val_ppl  = float(np.exp(val_loss)) if total_tok > 0 else float(\"inf\")\n",
    "val_acc  = correct / max(1, total_tok)\n",
    "\n",
    "print(f\"✅ Валид. метрики: Loss={val_loss:.4f} | PPL={val_ppl:.2f} | Acc={val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f45b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Сохранено в: C:\\Users\\User\\Desktop\\ИТМО\\Проект\\работа с базой DT_Curated\\chemberta_crossattn_rna\n"
     ]
    }
   ],
   "source": [
    "# ==== сохранение ====\n",
    "out_dir = DATA_DIR / \"chemberta_crossattn_rna\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "# сохраним веса и токенизатор в формате HF\n",
    "torch.save(model.state_dict(), out_dir / \"pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print(f\"\\n💾 Сохранено в: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12c5c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self,\n",
    "             tokenizer,\n",
    "             rna: torch.Tensor,\n",
    "             max_new_tokens: int = 64,\n",
    "             prefix: str = None,\n",
    "             temperature: float = 1.0,\n",
    "             top_k: int = None,\n",
    "             top_p: float = None):\n",
    "    self.eval()\n",
    "    device = next(self.parameters()).device\n",
    "\n",
    "    # BOS/префикс\n",
    "    if prefix is None:\n",
    "        bos_id = tokenizer.bos_token_id\n",
    "        if bos_id is None:\n",
    "            # для Roberta-подобных часто <s> как bos\n",
    "            bos_id = tokenizer.convert_tokens_to_ids(\"<s>\") if \"<s>\" in tokenizer.get_vocab() else tokenizer.cls_token_id\n",
    "        ids = torch.tensor([[bos_id]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        enc = tokenizer(prefix, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN, add_special_tokens=True)\n",
    "        ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    rna = rna.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            attn_mask = (ids != (tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0)).long()\n",
    "            logits = self.forward(ids, attn_mask, rna.expand(ids.size(0), -1))\n",
    "            next_logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "\n",
    "            # top-k\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, next_logits.size(-1))\n",
    "                v, _ = torch.topk(next_logits, k)\n",
    "                cut = v[:, [-1]]\n",
    "                next_logits[next_logits < cut] = -float(\"inf\")\n",
    "\n",
    "            # top-p (nucleus)\n",
    "            if top_p is not None and 0 < top_p < 1:\n",
    "                sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)\n",
    "                probs = torch.softmax(sorted_logits, dim=-1)\n",
    "                cumprobs = torch.cumsum(probs, dim=-1)\n",
    "                # маскируем всё после порога p\n",
    "                sorted_logits[cumprobs > top_p] = -float(\"inf\")\n",
    "                # возвращаем в исходный порядок\n",
    "                next_logits = torch.zeros_like(next_logits).scatter(1, sorted_idx, sorted_logits)\n",
    "\n",
    "            probs = torch.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)  # (B,1)\n",
    "\n",
    "            ids = torch.cat([ids, next_id], dim=1)\n",
    "\n",
    "            # остановка по EOS\n",
    "            if tokenizer.eos_token_id is not None and (next_id == tokenizer.eos_token_id).all():\n",
    "                break\n",
    "\n",
    "    return tokenizer.batch_decode(ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b00ff999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Validation Loss: 6.9463\n",
      "📉 Perplexity: 1039.2538\n",
      "🎯 Token Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# ==== валидация (метрики) ====\n",
    "def evaluate(model, dataloader, loss_f):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention = batch[\"attention_mask\"].to(DEVICE)\n",
    "            rna       = batch[\"rna\"].to(DEVICE)\n",
    "            labels    = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention, rna)\n",
    "            logits_shift = logits[:, :-1, :].contiguous()\n",
    "            labels_shift = labels[:, 1:].contiguous()\n",
    "\n",
    "            loss = loss_f(logits_shift.view(-1, logits_shift.size(-1)),\n",
    "                          labels_shift.view(-1))\n",
    "\n",
    "            mask = labels_shift != -100\n",
    "            n_tok = mask.sum().item()\n",
    "            total_loss += loss.item() * n_tok\n",
    "            total_tokens += n_tok\n",
    "\n",
    "            preds = logits_shift.argmax(dim=-1)\n",
    "            correct += ((preds == labels_shift) & mask).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 50 else float(\"inf\")\n",
    "    acc = correct / max(1, total_tokens)\n",
    "    return avg_loss, ppl, acc\n",
    "\n",
    "val_loss, val_ppl, val_acc = evaluate(model, val_dl, loss_f)\n",
    "print(f\"\\n📈 Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"📉 Perplexity: {val_ppl:.4f}\")\n",
    "print(f\"🎯 Token Accuracy: {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adf0c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Пробная генерация:\n",
      "CCCCCC~�ccncFCCCCNCcncscSHCCCCOcnncCCOCCOC��snSHSH�3CCCOCCCCOCCNC)/ncscCCOCCOC\u0007t�RCCONC�CCOcjCOP�OCCCnCSCCCNCOCCnCSCCOOCCCnTCSCCOC�CCOCCNCCCOcSHccccnRccoc�TNCCCCCC��OCCCNCccccn��43ClCCBrCCNCCCCCCCSCCOC�ccoc\n"
     ]
    }
   ],
   "source": [
    "# ==== быстрая генерация для проверки ====\n",
    "model.eval()\n",
    "rna_t = torch.tensor(embs_val[0], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "sample = model.generate(\n",
    "    tokenizer,\n",
    "    rna=rna_t,\n",
    "    max_new_tokens=64,\n",
    "    prefix=None,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(\"\\n🧪 Пробная генерация:\")\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
